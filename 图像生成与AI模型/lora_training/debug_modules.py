#!/usr/bin/env python3
"""
è°ƒè¯•è„šæœ¬ï¼šæŸ¥çœ‹UNetçš„æ¨¡å—åç§°
"""

import torch
from diffusers import StableDiffusionPipeline

def debug_unet_modules():
    """æŸ¥çœ‹UNetçš„æ¨¡å—åç§°"""
    print("ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")
    
    pipe = StableDiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float16,
        safety_checker=None
    )
    
    print("ğŸ” æŸ¥çœ‹UNetæ¨¡å—åç§°...")
    attention_modules = []
    linear_modules = []
    
    for name, module in pipe.unet.named_modules():
        if "attn" in name:
            attention_modules.append(name)
            print(f"æ³¨æ„åŠ›æ¨¡å—: {name}")
            if hasattr(module, "to_q"):
                print(f"  - æœ‰ to_q: {type(module.to_q)}")
            if hasattr(module, "to_k"):
                print(f"  - æœ‰ to_k: {type(module.to_k)}")
            if hasattr(module, "to_v"):
                print(f"  - æœ‰ to_v: {type(module.to_v)}")
            if hasattr(module, "to_out"):
                print(f"  - æœ‰ to_out: {type(module.to_out)}")
        
        if isinstance(module, torch.nn.Linear):
            linear_modules.append(name)
            if len(linear_modules) <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"çº¿æ€§å±‚: {name} -> {module}")
    
    print(f"\næ€»å…±æ‰¾åˆ° {len(attention_modules)} ä¸ªæ³¨æ„åŠ›æ¨¡å—")
    print(f"æ€»å…±æ‰¾åˆ° {len(linear_modules)} ä¸ªçº¿æ€§å±‚")

if __name__ == "__main__":
    debug_unet_modules() 