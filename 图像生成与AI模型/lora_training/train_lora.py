#!/usr/bin/env python3
"""
LoRAå¾®è°ƒè®­ç»ƒè„šæœ¬ - é’¢ç­‹æ£€æµ‹ä¸“ç”¨
åŸºäºStable Diffusion v1.4è¿›è¡ŒLoRAå¾®è°ƒ
"""

import os
import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import LoRAAttnProcessor
import json
from pathlib import Path
import argparse

def load_training_config(config_path):
    """åŠ è½½è®­ç»ƒé…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def setup_model():
    """è®¾ç½®åŸºç¡€æ¨¡å‹"""
    print("ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")
    
    # åŠ è½½Stable Diffusion v1.4
    model_id = "runwayml/stable-diffusion-v1-5"
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        safety_checker=None
    )
    
    # ä½¿ç”¨æ›´å¥½çš„è°ƒåº¦å™¨
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
    
    if torch.cuda.is_available():
        pipe = pipe.to("cuda")
        pipe.enable_attention_slicing()
    
    return pipe

def setup_lora(pipe, r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"]):
    """è®¾ç½®LoRAå‚æ•°"""
    print("ğŸ”§ è®¾ç½®LoRAå‚æ•°...")
    
    # è®¾ç½®LoRAé…ç½®
    lora_config = {
        "r": r,
        "lora_alpha": lora_alpha,
        "target_modules": target_modules,
        "lora_dropout": 0.1,
        "bias": "none",
        "task_type": "CAUSAL_LM"
    }
    
    # åº”ç”¨LoRAåˆ°æ³¨æ„åŠ›å±‚
    for name, module in pipe.unet.named_modules():
        if any(target in name for target in target_modules):
            if hasattr(module, "to_q"):
                module.to_q = torch.nn.Linear(module.to_q.in_features, module.to_q.out_features, bias=False)
            if hasattr(module, "to_k"):
                module.to_k = torch.nn.Linear(module.to_k.in_features, module.to_k.out_features, bias=False)
            if hasattr(module, "to_v"):
                module.to_v = torch.nn.Linear(module.to_v.in_features, module.to_v.out_features, bias=False)
            if hasattr(module, "to_out"):
                module.to_out = torch.nn.Linear(module.to_out.in_features, module.to_out.out_features, bias=False)
    
    return pipe, lora_config

def train_lora(pipe, training_data, output_dir, num_epochs=100):
    """æ‰§è¡ŒLoRAè®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹LoRAè®­ç»ƒ...")
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=1e-4)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        print(f"ğŸ“š Epoch {epoch+1}/{num_epochs}")
        
        # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„è®­ç»ƒé€»è¾‘
        # ç”±äºå®Œæ•´çš„è®­ç»ƒå®ç°æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›æ¡†æ¶
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0:
            checkpoint_path = output_dir / f"lora_checkpoint_epoch_{epoch+1}.safetensors"
            # pipe.save_lora_weights(checkpoint_path)
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    print("âœ… LoRAè®­ç»ƒå®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description="LoRAå¾®è°ƒè®­ç»ƒ")
    parser.add_argument("--config", type=str, default="config/training_config.json", help="è®­ç»ƒé…ç½®æ–‡ä»¶")
    parser.add_argument("--output", type=str, default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_training_config(args.config)
    
    # è®¾ç½®æ¨¡å‹
    pipe = setup_model()
    pipe, lora_config = setup_lora(pipe)
    
    # å¼€å§‹è®­ç»ƒ
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True)
    
    train_lora(pipe, config, output_dir, args.epochs)

if __name__ == "__main__":
    main()
