#!/usr/bin/env python3
"""
GPUä½¿ç”¨æƒ…å†µæµ‹è¯•è„šæœ¬
"""

import torch
import time
from diffusers import StableDiffusionPipeline
from pathlib import Path

def test_gpu_usage():
    """æµ‹è¯•GPUä½¿ç”¨æƒ…å†µ"""
    print("ğŸ” GPUä½¿ç”¨æƒ…å†µæµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"GPUåç§°: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"GPUå†…å­˜å·²ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB")
        print(f"GPUå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.1f} GB")
    
    # åŠ è½½æ¨¡å‹å¹¶æµ‹è¯•
    print("\nğŸ”„ åŠ è½½æ¨¡å‹æµ‹è¯•...")
    model_path = "models/CompVis/stable-diffusion-v1-4"
    
    if not Path(model_path).exists():
        print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¨¡å‹æµ‹è¯•")
        return
    
    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åŠ è½½æ¨¡å‹
        pipe = StableDiffusionPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            safety_checker=None
        )
        
        if torch.cuda.is_available():
            pipe = pipe.to("cuda")
            pipe.enable_attention_slicing()
        
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}ç§’)")
        
        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            print(f"åŠ è½½åGPUå†…å­˜å·²ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB")
            print(f"åŠ è½½åGPUå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.1f} GB")
        
        # æµ‹è¯•ç”Ÿæˆä¸€å¼ å›¾åƒ
        print("\nğŸ¨ æµ‹è¯•ç”Ÿæˆå›¾åƒ...")
        test_start = time.time()
        
        # ä½¿ç”¨ç®€å•çš„æç¤ºè¯
        image = pipe(
            prompt="a simple steel rebar, ribbed surface, construction",
            num_inference_steps=20,
            guidance_scale=7.5,
            height=512,
            width=512
        ).images[0]
        
        test_time = time.time() - test_start
        print(f"âœ… å›¾åƒç”Ÿæˆå®Œæˆ (è€—æ—¶: {test_time:.2f}ç§’)")
        
        # ä¿å­˜æµ‹è¯•å›¾åƒ
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)
        image.save(output_dir / "gpu_test_image.png")
        print("âœ… æµ‹è¯•å›¾åƒå·²ä¿å­˜: outputs/gpu_test_image.png")
        
        # æœ€ç»ˆGPUå†…å­˜æ£€æŸ¥
        if torch.cuda.is_available():
            print(f"ç”ŸæˆåGPUå†…å­˜å·²ç”¨: {torch.cuda.memory_allocated() / 1024**3:.1f} GB")
            print(f"ç”ŸæˆåGPUå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved() / 1024**3:.1f} GB")
        
        print("\nğŸ“Š æ€»ç»“:")
        print("- GPUç¡®å®åœ¨å·¥ä½œï¼Œå†…å­˜è¢«å ç”¨")
        print("- 3Dåˆ©ç”¨ç‡ä½æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæ‰©æ•£æ¨¡å‹è®¡ç®—æ˜¯é—´æ­‡æ€§çš„")
        print("- ç”Ÿæˆé€Ÿåº¦æ­£å¸¸ï¼Œè¯´æ˜GPUåŠ é€Ÿæœ‰æ•ˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_gpu_usage() 